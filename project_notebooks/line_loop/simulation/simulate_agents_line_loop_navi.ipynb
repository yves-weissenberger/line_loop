{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build simple agents that perform navigation on the line and the loop\n",
    "\n",
    "Agents we want to study\n",
    "<ul stlye=\"list-style: square\">\n",
    "    <li> Go inwards </li>\n",
    "    <li> Simple Q-learning agent </li>\n",
    "    <li> Model based agent </li>\n",
    "    <li> Go up and down stereotyped </li>\n",
    "    <li> Fully random decisions </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import seaborn\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "seaborn.set(font_scale=1.5,style='ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_agent(object):\n",
    "    \"\"\"\n",
    "    This is a generic class that other agents (with different policies)\n",
    "    inherit from. This class implements the basic task_logic\n",
    "    \"\"\"\n",
    "    def __init__(self,learning_params,task_params=None):\n",
    "        \"\"\" \n",
    "        For simplicity, in the first instance, will assume that\n",
    "        \n",
    "        Arguments:\n",
    "        =======================\n",
    "        \n",
    "        task_params (dict):  dictionary containing parameters that are required for running of the task\n",
    "        \n",
    "        learning_params (dict): parameters necessary for specifying an \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.task_params is None:\n",
    "            task_params = {'len_graph':9,\n",
    "                           'graph_type':'line',\n",
    "                           'rew_locs_state':list(range(9)),\n",
    "                           'n_rewards_tot': 1000,\n",
    "                           'rewards_pre_switch': 20,\n",
    "                           'reward_switch_p': 0.2,\n",
    "                            }\n",
    "\n",
    "        \n",
    "        self.learning_params = learning_params\n",
    "        self.task_params = task_params\n",
    "        \n",
    "        \n",
    "        #helper params\n",
    "        self._all_states = list(range(self.task_params['len_graph']))\n",
    "        \n",
    "\n",
    "        self._init_task_variables()\n",
    "        self._init_stores()\n",
    "        \n",
    "\n",
    "        \n",
    "    def _init_task_variables(self):\n",
    "        #initialise variables\n",
    "        self.forced = False\n",
    "        self.reward_location = None\n",
    "        self.current_state = None\n",
    "        self.current_port = None\n",
    "        self.current_reward_location = None\n",
    "        self.available_states = []\n",
    "        self.choices_since_reward = 0\n",
    "        self.n_rewards_at_loc = 0  \n",
    "        \n",
    "    def _init_stores(self):\n",
    "        #this is the information to extract at the end to feed into\n",
    "        #the standard analysis pipeline\n",
    "        self.state_seq = []\n",
    "        self.rew_list = []\n",
    "        self.port_seq = []\n",
    "        self.forced_seq = []\n",
    "\n",
    "    def set_available_states(self):\n",
    "        \"\"\" updates available_states variables\"\"\"\n",
    "        if self.current_state==0:\n",
    "            if self.task_params['graph_type']=='line':\n",
    "                self.available_states = [1]\n",
    "            else:\n",
    "                self.available_states = [self.task_params['len_graph']-1,1]\n",
    "        elif self.current_state==(self.task_params['len_graph']-1):\n",
    "            if self.task_params['graph_type']=='line':\n",
    "                self.available_states = [self.task_params['len_graph']-2]\n",
    "            else:\n",
    "                self.available_states = [self.task_params['len_graph']-2,0]\n",
    "        else:\n",
    "            self.available_states = [self.current_state-1,self.current_state+1]\n",
    "\n",
    "    \n",
    "    def init_trial(self):\n",
    "        \"\"\" Initialise the task\"\"\"\n",
    "        self.current_state = np.random.choice(self.task_params['rew_locs_state'])\n",
    "        if self.reward_location is None:\n",
    "            self.update_reward_location()\n",
    "\n",
    "        \n",
    "    def run(self,n_trials=1e5):\n",
    "        \"\"\" Run behaviour \"\"\"\n",
    "        \n",
    "    \n",
    "    def update_state(self):\n",
    "        \"\"\" Received choice of 0 or 1\"\"\"\n",
    "        self.current_state = self.policy(self.available_states)\n",
    "        if self.current_state==self.reward_location:\n",
    "            self.choices_since_reward = 0\n",
    "            self.n_rewards_at_loc += 1\n",
    "            self.choices_since_reward += 1\n",
    "        #pass\n",
    "    \n",
    "    def update_reward_location(self):\n",
    "        self.reward_location = np.random.choice(self.task_params['rew_locs_state'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def policy(self):\n",
    "        next_state = np.random.choice(self.available_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class towards_middle_agent(base_agent):\n",
    "    \n",
    "    def __init__(self,learning_params,task_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner(base_agent):\n",
    "    \n",
    "    def __init__(self,learning_params,task_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_based_agent(base_agent):\n",
    "    \n",
    "    def __init__(self,learning_params,task_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
