{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build simple agents that perform navigation on the line and the loop\n",
    "\n",
    "Agents we want to study\n",
    "<ul stlye=\"list-style: square\">\n",
    "    <li> Go inwards </li>\n",
    "    <li> Simple Q-learning agent </li>\n",
    "    <li> Model based agent </li>\n",
    "    <li> Go up and down stereotyped </li>\n",
    "    <li> Fully random decisions </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import seaborn\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "seaborn.set(font_scale=1.5,style='ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modulo_distance(target_state,state,nStates=6):\n",
    "    return np.min(np.abs([target_state-state,target_state-(state+nStates),target_state-(state-nStates)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_agent(object):\n",
    "    \"\"\"\n",
    "    This is a generic class that other agents (with different policies)\n",
    "    inherit from. This class implements the basic task_logic\n",
    "    \"\"\"\n",
    "    def __init__(self,learning_params=None,task_params=None):\n",
    "        \"\"\" \n",
    "        For simplicity, in the first instance, will assume that\n",
    "        \n",
    "        Arguments:\n",
    "        =======================\n",
    "        \n",
    "        task_params (dict):  dictionary containing parameters that are required for running of the task\n",
    "        \n",
    "        learning_params (dict): parameters necessary for specifying and updating the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if task_params is None:\n",
    "            task_params = {'len_graph':9,\n",
    "                           'graph_type':'line',\n",
    "                           'rew_locs_session':list(range(9)),\n",
    "                           'rewards_pre_switch': 20,\n",
    "                           'reward_switch_p': 0.2,\n",
    "                           'choices_pre_forced': 20\n",
    "                            }\n",
    "\n",
    "        \n",
    "        self.learning_params = learning_params\n",
    "        self.task_params = task_params\n",
    "        \n",
    "        \n",
    "        #helper params\n",
    "        self._all_states = list(range(self.task_params['len_graph']))\n",
    "        \n",
    "\n",
    "        self._init_task_variables()\n",
    "        self._init_stores()\n",
    "        \n",
    "\n",
    "        \n",
    "    def _init_task_variables(self):\n",
    "        #initialise variables\n",
    "        self.forced = False\n",
    "        self.reward_location = None\n",
    "        self.current_state = None\n",
    "        self.current_port = None\n",
    "        self.current_reward_location = None\n",
    "        self.available_states = []\n",
    "        self.choices_since_reward = 0\n",
    "        self.n_rewards_at_loc = 0\n",
    "        self.n_reward_total = 0\n",
    "        \n",
    "    def _init_stores(self):\n",
    "        #this is the information to extract at the end to feed into\n",
    "        #the standard analysis pipeline\n",
    "        self.state_seq = []\n",
    "        self.rew_list = []\n",
    "        self.port_seq = []\n",
    "        self.forced_seq = []\n",
    "\n",
    "    def set_available_states(self):\n",
    "        \"\"\" updates available_states variables\"\"\"\n",
    "        \n",
    "        #if bottom edge\n",
    "        if self.current_state==0:\n",
    "            if self.task_params['graph_type']=='line':\n",
    "                self.available_states = [1]\n",
    "            else:\n",
    "                self.available_states = [self.task_params['len_graph']-1,1]\n",
    "\n",
    "        #elif top edge\n",
    "        elif self.current_state==(self.task_params['len_graph']-1):\n",
    "\n",
    "            if self.task_params['graph_type']=='line':\n",
    "                self.available_states = [self.task_params['len_graph']-2]\n",
    "            else:\n",
    "                self.available_states = [self.task_params['len_graph']-2,0]\n",
    "\n",
    "        #with states either side\n",
    "        else:\n",
    "            self.available_states = [self.current_state-1,self.current_state+1]\n",
    "\n",
    "        #if its a forced trial\n",
    "        if self.choices_since_reward>self.task_params['choices_pre_forced']:\n",
    "            self.forced_seq.append(True)\n",
    "            self.forced = True\n",
    "            #if this is a forced trial\n",
    "            if len(self.available_states)>1:\n",
    "                \n",
    "                if self.task_params['graph_type']=='line':\n",
    "                    best_ix = np.argmin([np.abs(self.available_states[0]-self.reward_location),\n",
    "                                          np.abs(self.available_states[1]-self.reward_location)])\n",
    "                    self.available_states = [self.available_states[best_ix]]\n",
    "                else:\n",
    "                    best_ix = np.argmin([get_modulo_distance(self.reward_location,self.available_states[1]),\n",
    "                                          get_modulo_distance(self.reward_location,self.available_states[1])])\n",
    "                    self.available_states = [self.available_states[best_ix]]\n",
    "\n",
    "        else:\n",
    "            self.forced_seq.append(False)\n",
    "\n",
    "    \n",
    "    def init_trial(self):\n",
    "        \"\"\" Initialise the task\"\"\"\n",
    "        if ((self.reward_location is None) or \n",
    "            (self.n_rewards_at_loc>self.task_params['rewards_pre_switch'] and\n",
    "                np.random.uniform(0,1)>self.task_params['reward_switch_p'])):\n",
    "            self.update_reward_location()\n",
    "        \n",
    "            \n",
    "\n",
    "        self.current_state = np.random.choice(self.poss_starting_states)\n",
    "        self.set_available_states()\n",
    "        #print(self.curr)\n",
    "        #print(self.available_states)\n",
    "        self.state_seq.append(self.current_state)\n",
    "        self.port_seq.append(self.current_port)\n",
    "        self.rew_list.append(False)\n",
    "\n",
    "        \n",
    "    def run(self,n_trials=3e3):\n",
    "        \"\"\" Run behaviour \"\"\"\n",
    "        self.init_trial()\n",
    "        #if self.task_params\n",
    "        #print(n_trials)\n",
    "        rew = False\n",
    "        while self.n_reward_total<n_trials:\n",
    "            if rew:\n",
    "                self.update_policy(end_of_trial=True)\n",
    "                self.init_trial()\n",
    "                \n",
    "\n",
    "            #print(self.available_states)\n",
    "            rew = self.update_state()\n",
    "            self.update_policy(end_of_trial=False)\n",
    "            self.state_seq.append(self.current_state)\n",
    "            self.port_seq.append(self.current_port)\n",
    "            self.rew_list.append(rew)\n",
    "                    \n",
    "        \n",
    "    \n",
    "    def update_state(self):\n",
    "        \"\"\" Received choice of 0 or 1\"\"\"\n",
    "        self.do_policy()\n",
    "        self.set_available_states()\n",
    "\n",
    "        rew = False\n",
    "        if self.current_state==self.reward_location:\n",
    "            self.choices_since_reward = 0\n",
    "            self.n_rewards_at_loc += 1\n",
    "            self.choices_since_reward =0 \n",
    "            self.n_reward_total += 1\n",
    "            #self.rew_list.append(True)\n",
    "            rew = True\n",
    "        else:\n",
    "            self.choices_since_reward += 1\n",
    "\n",
    "        return rew\n",
    "            \n",
    "    \n",
    "    def update_reward_location(self):\n",
    "        self.n_rewards_at_loc = 0\n",
    "        self.reward_location = np.random.choice(self.task_params['rew_locs_session'])\n",
    "        self.poss_starting_states = [i for i in self.task_params['rew_locs_session'] if i!=self.reward_location]\n",
    "\n",
    "    \n",
    "    def do_policy(self):\n",
    "        pass #return None\n",
    "    def update_policy(self,end_of_trial=False):\n",
    "        \"\"\" \n",
    "        Use this function when the behavioural policy is actually updated \n",
    "        (e.g. Q-learning),not if the behavioural policy involves inference.\n",
    "        use end_of_trial signal for episodic updates\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffusion_agent(base_agent):\n",
    "    \"\"\" This agent simply makes random choices at each time point\"\"\"\n",
    "    def __init__(self,learning_params=None,task_params=None):\n",
    "        super().__init__(learning_params,task_params)\n",
    "    \n",
    "    def do_policy(self):\n",
    "        self.current_state = np.random.choice(self.available_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "class towards_middle_agent(base_agent):\n",
    "    \"\"\" This is an agent where you have two policies. It selects between these\n",
    "        two policies based on how many states are in each direction. Right now\n",
    "        this is the simplest possible implementation that samples a direction and \n",
    "        sticks with it. You could imagine one where you essentially calculate values based on \n",
    "        number of states available and then make a direction decision at each point. \n",
    "        Additionally, you can just add stochasticity at all points (i.e. go in direction and change direction)\n",
    "    \"\"\"\n",
    "    def __init__(self,learning_params=None,task_params=None):\n",
    "        super().__init__(learning_params,task_params)\n",
    "        \n",
    "    \n",
    "    def do_policy(self):\n",
    "        if self.task_params['graph_type']=='line':\n",
    "            \n",
    "            if self.choices_since_reward==0:\n",
    "                self.direction = int(np.abs(self.current_state-9)>np.abs(self.current_state-0)) #\n",
    "                if len(self.available_states)==2:\n",
    "                    self.current_state = self.available_states[self.direction]\n",
    "                else:\n",
    "                    self.current_state = self.available_states[0]\n",
    "                #    self.direction = 1\n",
    "                #else:\n",
    "                #    self.direction = -1\n",
    "            else:\n",
    "                if len(self.available_states)==2:\n",
    "                    self.current_state = self.available_states[self.direction]\n",
    "                else:\n",
    "                    if not self.forced:\n",
    "                        if self.current_state==0: self.direction = 1\n",
    "                        elif self.current_state==(self.task_params['len_graph']-1): self.direction = 0\n",
    "                            \n",
    "                    self.current_state = self.available_states[0]\n",
    "        else:\n",
    "            raise Exception('No idea what could happen on the loop here')\n",
    "            #self.current_state = np.random.choice(self.available_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner(base_agent):\n",
    "    \"\"\" Simple Q-learning agent\"\"\"\n",
    "    def __init__(self,learning_params,task_params):\n",
    "        super().__init__(learning_params,task_params)\n",
    "        self.sigmoid = lambda x: 1/(1+np.exp(-(x[1]-x[0])))\n",
    "        self.decisions_pre_rew = []\n",
    "        self.Q_values = np.zeros(self.task_para)\n",
    "        \n",
    "    def do_policy(self):\n",
    "        pass\n",
    "    \n",
    "    def update_policy(self,end_of_trial=False):\n",
    "        #\n",
    "        if end_of_trial:\n",
    "            self.gammas = [G**i for i in reversed(range(len(decs)))]\n",
    "            Q_new[decs[dix],up] = alpha*Q_new[decs[dix],up] + (1-alpha)*gammas[dix]*rVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_based_agent(base_agent):\n",
    "    \"\"\" This is a perfect model based agent \"\"\"\n",
    "    def __init__(self,learning_params=None,task_params=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.known_reward_location = False\n",
    "    \n",
    "    def exploration_policy(self):\n",
    "        \"\"\" explore the environment randomly\"\"\"\n",
    "        self.current_state = np.random.choice(self.available_states)\n",
    "\n",
    "    def do_policy(self):\n",
    "        if not self.known_reward_location:\n",
    "            self.exploration_policy()\n",
    "        else:\n",
    "            \n",
    "            if len(self.available_states)==2:\n",
    "\n",
    "                if self.task_params['graph_type']=='line':\n",
    "\n",
    "                    d0 = np.abs(self.known_reward_location-self.available_states[0])\n",
    "                    d1 = np.abs(self.known_reward_location-self.available_states[1])\n",
    "                else:\n",
    "                    d0 = get_modulo_distance(self.known_reward_location,\n",
    "                                            self.available_states[0],\n",
    "                                            nStates=self.task_params['len_graph'])\n",
    "                    d1 = get_modulo_distance(self.known_reward_location,\n",
    "                                            self.available_states[1],\n",
    "                                            nStates=self.task_params['len_graph'])\n",
    "                    \n",
    "                self.current_state = self.available_states[np.argmin([d0,d1])]\n",
    "\n",
    "            else:\n",
    "                self.current_state = self.available_states[0]\n",
    "                \n",
    "    def update_policy(self,end_of_trial):\n",
    "        print(1)\n",
    "        if end_of_trial:\n",
    "            if self.known_reward_location is None:\n",
    "                self.known_reward_location = self.current_state\n",
    "        #else:\n",
    "            elif (self.current_state!=self.known_reward_location):\n",
    "                self.known_reward_location = self.current_state\n",
    "        else:\n",
    "            if (self.current_state!=self.known_reward_location):\n",
    "                self.known_reward_location = None\n",
    "                \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.available_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.set_available_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.reward_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "aa = Model_based_agent()\n",
    "#aa.task_params['graph_type'] = 'loop'\n",
    "aa.run(n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa.forced_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cst = 1\n",
    "[np.abs(cst-0),np.abs(cst-9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner(base_agent):\n",
    "    \n",
    "    def __init__(self,learning_params,task_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_based_agent(base_agent):\n",
    "    \n",
    "    def __init__(self,learning_params,task_params):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
